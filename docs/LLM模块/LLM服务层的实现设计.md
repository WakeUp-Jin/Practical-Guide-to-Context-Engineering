
# LLMæœåŠ¡å±‚çš„å®ç°è®¾è®¡

è‡ªå·±åœ¨é¡¹ç›®ä¸­å®ç°ä¸€ä¸ª LLM çš„æœåŠ¡å•†é›†æˆçš„æ€è·¯ä¸»è¦æ˜¯ï¼šè®¾è®¡ LLM æœåŠ¡çš„ç»Ÿä¸€æ¥å£ï¼Œå·¥å‚å‡½æ•°çš„åˆ›å»ºï¼Œæ¶ˆæ¯çš„æ ¼å¼åŒ–è¿™ä¸‰ç§ï¼Œ

1. æ¯ä¸€ç§ç±»å‹çš„ä¾›åº”å•†**è¾“å‡ºçš„ç»“æœçš„æ ¼å¼éƒ½ä¸ä¸€æ ·**ï¼Œæ‰€ä»¥éœ€è¦å°†è¿™äº›ä¸åŒçš„å¤–éƒ¨è¾“å‡ºæ ¼å¼è½¬æ¢ä¸ºå†…éƒ¨ä¼ é€’çš„æ¶ˆæ¯æ ¼å¼
2. æ¯ä¸€ç§ç±»å‹çš„ä¾›åº”å•†**è¦æ±‚çš„å‚æ•°çš„ message å’Œ tool çš„æ ¼å¼ä¹Ÿä¸ä¸€æ ·**ï¼Œæ‰€ä»¥ä¹Ÿéœ€è¦æ ¹æ®ä¸åŒçš„ä¾›åº”å•†å°†å†…éƒ¨çš„æ¶ˆæ¯æ ¼å¼å’Œå·¥å…·æ ¼å¼è½¬æ¢ä¸ºè¿™äº›ä¾›åº”å•†éœ€è¦çš„å‚æ•°æ ¼å¼



è¿™æ ·è‡ªå®šä¹‰å°è£…å®ç°çš„ä¼˜åŠ¿æ˜¯ï¼š

+ å®šåˆ¶åŒ–åŠŸèƒ½ï¼šæ·»åŠ é‡è¯•æœºåˆ¶ã€ä¸Šä¸‹æ–‡å‹ç¼©ã€ç›´æ¥ç”Ÿæˆæ¨¡å¼ç­‰ç‰¹å®šåŠŸèƒ½
+ ç³»ç»Ÿé›†æˆï¼šå°† LLM çš„è°ƒç”¨ä¸ç³»ç»Ÿçš„å…¶ä»–ç»„ä»¶æ— ç¼é›†æˆ
+ è½»é‡çº§ä¸æ€§èƒ½ï¼šæ›´åŠ è½»é‡çº§ï¼ŒåªåŒ…å«å¿…è¦çš„åŠŸèƒ½ï¼Œæ€§èƒ½å¼€é”€æ›´å°
+ é¿å…ä¾›åº”å•†é”å®šï¼šæ›´åŠ çš„çµæ´»ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°æ›¿æ¢æˆ–ä¿®æ”¹åº•å±‚å®ç°



é‚£ä¹ˆæ¶æ„è®¾è®¡è§£å†³çš„é—®é¢˜æ˜¯ï¼š

1. å¤šä¾›åº”å•†çš„æ”¯æŒï¼šé€šè¿‡å·¥å‚æ¨¡å¼å’Œç»Ÿä¸€æ¥å£ï¼Œç³»ç»Ÿå¯ä»¥è½»æ¾æ”¯æŒå¤šä¸ªLLMæä¾›å•†ï¼Œå¹¶æ ¹æ®é…ç½®åŠ¨æ€åˆ‡æ¢ã€‚
2. ä¸Šä¸‹æ–‡ç®¡ç†ï¼š`<u>ContextManager</u>`é€šè¿‡æ¶ˆæ¯æŒä¹…åŒ–ã€ä»¤ç‰Œæ„ŸçŸ¥å‹ç¼©å’Œæ¶ˆæ¯æ ¼å¼åŒ–è§£å†³äº†ä¸Šä¸‹æ–‡ç®¡ç†çš„å¤æ‚æ€§ã€‚
3. å·¥å…·è°ƒç”¨ä¼˜åŒ–ï¼šå°è£…å±‚æä¾›äº†ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ¥å£ï¼Œå¤„ç†ä¸åŒLLMæä¾›å•†çš„å·¥å…·è°ƒç”¨å·®å¼‚ã€‚
4. é”™è¯¯å¤„ç†ä¸å¼¹æ€§ï¼šå°è£…å±‚æä¾›äº†å¥å£®çš„é”™è¯¯å¤„ç†å’Œå¼¹æ€§æœºåˆ¶ï¼ŒåŒ…æ‹¬é‡è¯•é€»è¾‘ã€ä¸Šä¸‹æ–‡é•¿åº¦å¤„ç†å’Œå·¥å…·æ‰§è¡Œé”™è¯¯å¤„ç†



æ•´ä½“çš„æ¶æ„è®¾è®¡çš„å…³ç³»å›¾å¦‚ä¸‹ï¼š

Excalidraw æ–‡ä»¶ï¼š[https://gcntfv628ebr.feishu.cn/file/EdQObfoV0oR6IHx48CGcPWPnnEb](https://gcntfv628ebr.feishu.cn/file/EdQObfoV0oR6IHx48CGcPWPnnEb)

![LLMæœåŠ¡å±‚çš„å®ç°è®¾è®¡](./image/image%20(30).png)



## ä¸€ã€LLM æœåŠ¡æ¥å£ç»Ÿä¸€è®¾è®¡
LLM æœåŠ¡æ¥å£å®šä¹‰äº†ç»Ÿä¸€çš„å¥‘çº¦ï¼ŒåŒ…å«å››ä¸ªæ ¸å¿ƒæ–¹æ³•

+ **generate()**: ä¸»è¦ç”Ÿæˆæ–¹æ³•ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œå›¾åƒè¾“å…¥
+ **directGenerate**(): ç›´æ¥ç”Ÿæˆï¼Œç»•è¿‡å¯¹è¯ä¸Šä¸‹æ–‡
+ **getAllTools**(): è·å–æ‰€æœ‰å¯ç”¨å·¥å…·
+ **getConfig**(): è·å–æœåŠ¡é…ç½®ä¿¡æ¯

```typescript
export interface ILLMService {
	generate(userInput: string, imageData?: ImageData, stream?: boolean): Promise<string>;
	directGenerate(userInput: string, systemPrompt?: string): Promise<string>;
	getAllTools(): Promise<ToolSet>;
	getConfig(): LLMServiceConfig;
}
```



è¿˜æœ‰ä¸€äº›åœ¨å…·ä½“å®ç°çš„æ—¶å€™çš„è¾…åŠ©æ–¹æ³•

+ getAIResponseWithRetriesï¼šé‡è¯•æœºåˆ¶ï¼Œå¤„ç†ç½‘ç»œæ³¢åŠ¨å’Œé”™è¯¯
+ formatToolsForProviderï¼šå·¥å…·æ ¼å¼è½¬æ¢
+ isRetryableErrorï¼šé”™è¯¯å¤„ç†

```typescript
  // é‡è¯•æœºåˆ¶ - å¤„ç†APIæ³¢åŠ¨å’Œé”™è¯¯
  private async getAIResponseWithRetries(tools: any[], userInput:string): Promise<{response: any}>

  // å·¥å…·æ ¼å¼è½¬æ¢ - é€‚é…ä¸åŒAPIæ ¼å¼
  private formatToolsForProvider(tools: ToolSet): any[]

  // é”™è¯¯å¤„ç† - å¯é€‰ï¼Œå¤æ‚æä¾›å•†éœ€è¦
  private isRetryableError?(status: number, errorType: string):boolean
  private calculateRetryDelay?(attempt: number, status: number):number
```



è¿˜æœ‰æ¯ä¸€ä¸ªå…·ä½“çš„ LLM æœåŠ¡ç±»éœ€è¦å®ç°çš„æ ¸å¿ƒå±æ€§

```typescript
  export class NewProviderService implements ILLMService {
      // å¿…éœ€çš„æ ¸å¿ƒç»„ä»¶
      private client: ProviderClient;           // æä¾›å•†SDKå®¢æˆ·ç«¯
      private model: string;                    // æ¨¡å‹åç§°
      private mcpManager: MCPManager;           // MCPå·¥å…·ç®¡ç†
      private contextManager: ContextManager;   // å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†  
      private maxIterations: number;            // æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°
      private unifiedToolManager?: UnifiedToolManager; // å¯é€‰ï¼šç»Ÿä¸€å·¥å…·ç®¡ç†
  }
```



ä¾‹å¦‚ openAIService çš„å®ç°

```typescript
import { ToolSet } from '../../../mcp/types.js';
import { MCPManager } from '../../../mcp/manager.js';
import { UnifiedToolManager, CombinedToolSet } from '../../tools/unified-tool-manager.js';
import { ContextManager } from '../messages/manager.js';
import { ImageData } from '../messages/types.js';
import { ILLMService, LLMServiceConfig } from './types.js';
import OpenAI from 'openai';
import { logger } from '../../../logger/index.js';
import { formatToolResult } from '../utils/tool-result-formatter.js';

export class OpenAIService implements ILLMService {
	private openai: OpenAI;
	private model: string;
	private mcpManager: MCPManager;
	private unifiedToolManager: UnifiedToolManager | undefined;
	private contextManager: ContextManager;
	private maxIterations: number;

	constructor(
		openai: OpenAI,
		model: string,
		mcpManager: MCPManager,
		contextManager: ContextManager,
		maxIterations: number = 5,
		unifiedToolManager?: UnifiedToolManager
	) {
		this.openai = openai;
		this.model = model;
		this.mcpManager = mcpManager;
		this.unifiedToolManager = unifiedToolManager;
		this.contextManager = contextManager;
		this.maxIterations = maxIterations;
	}
	async generate(userInput: string, imageData?: ImageData): Promise<string> {
		await this.contextManager.addUserMessage(userInput, imageData);

		// Use unified tool manager if available, otherwise fall back to MCP manager
		let formattedTools: any[];
		if (this.unifiedToolManager) {
			formattedTools = await this.unifiedToolManager.getToolsForProvider('openai');
		} else {
			const rawTools = await this.mcpManager.getAllTools();
			formattedTools = this.formatToolsForOpenAI(rawTools);
		}

		logger.silly(`Formatted tools: ${JSON.stringify(formattedTools, null, 2)}`);

		let iterationCount = 0;
		try {
			while (iterationCount < this.maxIterations) {
				iterationCount++;

				// Attempt to get a response, with retry logic
				const { message } = await this.getAIResponseWithRetries(formattedTools, userInput);

				// If there are no tool calls, we're done
				if (!message.tool_calls || message.tool_calls.length === 0) {
					const responseText = message.content || '';
					// Add assistant message to history
					await this.contextManager.addAssistantMessage(responseText);
					return responseText;
				}

				// Log thinking steps when assistant provides reasoning before tool calls
				if (message.content && message.content.trim()) {
					logger.info(`ğŸ’­ ${message.content.trim()}`);
				}

				// Add assistant message with tool calls to history
				await this.contextManager.addAssistantMessage(message.content, message.tool_calls);

				// Handle tool calls
				for (const toolCall of message.tool_calls) {
					logger.debug(`Tool call initiated: ${JSON.stringify(toolCall, null, 2)}`);
					logger.info(`ğŸ”§ Using tool: ${toolCall.function.name}`);
					const toolName = toolCall.function.name;
					let args: any = {};

					try {
						args = JSON.parse(toolCall.function.arguments);
					} catch (e) {
						logger.error(`Error parsing arguments for ${toolName}:`, e);
						await this.contextManager.addToolResult(toolCall.id, toolName, {
							error: `Failed to parse arguments: ${e}`,
						});
						continue;
					}

					// Execute tool
					try {
						let result: any;
						if (this.unifiedToolManager) {
							result = await this.unifiedToolManager.executeTool(toolName, args);
						} else {
							result = await this.mcpManager.executeTool(toolName, args);
						}

						// Display formatted tool result
						const formattedResult = formatToolResult(toolName, result);
						logger.info(`ğŸ“‹ Tool Result:\n${formattedResult}`);

						// Add tool result to message manager
						await this.contextManager.addToolResult(toolCall.id, toolName, result);
					} catch (error) {
						// Handle tool execution error
						const errorMessage = error instanceof Error ? error.message : String(error);
						logger.error(`Tool execution error for ${toolName}: ${errorMessage}`);

						// Add error as tool result
						await this.contextManager.addToolResult(toolCall.id, toolName, {
							error: errorMessage,
						});
					}
				}
			}

			// If we reached max iterations, return a message
			logger.warn(`Reached maximum iterations (${this.maxIterations}) for task.`);
			const finalResponse = 'Task completed but reached maximum tool call iterations.';
			await this.contextManager.addAssistantMessage(finalResponse);
			return finalResponse;
		} catch (error) {
			// Handle API errors
			const errorMessage = error instanceof Error ? error.message : String(error);
			logger.error(`Error in OpenAI service API call: ${errorMessage}`, { error });
			await this.contextManager.addAssistantMessage(`Error processing request: ${errorMessage}`);
			return `Error processing request: ${errorMessage}`;
		}
	}

	/**
	 * Direct generate method that bypasses conversation context
	 * Used for internal tool operations that shouldn't pollute conversation history
	 * @param userInput - The input to generate a response for
	 * @param systemPrompt - Optional system prompt to use
	 * @returns Promise<string> - The generated response
	 */
	async directGenerate(userInput: string, systemPrompt?: string): Promise<string> {
		try {
			logger.debug('OpenAIService: Direct generate call (bypassing conversation context)', {
				inputLength: userInput.length,
				hasSystemPrompt: !!systemPrompt,
			});

			// Create a minimal message array for direct API call
			const messages: any[] = [];

			if (systemPrompt) {
				messages.push({
					role: 'system',
					content: systemPrompt,
				});
			}

			messages.push({
				role: 'user',
				content: userInput,
			});

			// Make direct API call without adding to conversation context
			const response = await this.openai.chat.completions.create({
				model: this.model,
				messages: messages,
				// No tools for direct calls - this is for simple text generation
			});

			const responseText = response.choices[0]?.message?.content || '';

			logger.debug('OpenAIService: Direct generate completed', {
				responseLength: responseText.length,
			});

			return responseText;
		} catch (error) {
			const errorMessage = error instanceof Error ? error.message : String(error);
			logger.error('OpenAIService: Direct generate failed', {
				error: errorMessage,
				inputLength: userInput.length,
			});
			throw new Error(`Direct generate failed: ${errorMessage}`);
		}
	}

	async getAllTools(): Promise<ToolSet | CombinedToolSet> {
		if (this.unifiedToolManager) {
			return await this.unifiedToolManager.getAllTools();
		}
		return this.mcpManager.getAllTools();
	}

	getConfig(): LLMServiceConfig {
		return {
			provider: 'openai',
			model: this.model,
		};
	}

	// Helper methods
	private async getAIResponseWithRetries(
		tools: any[],
		userInput: string
	): Promise<{ message: any }> {
		let attempts = 0;
		const MAX_ATTEMPTS = 3;

		// Add a log of the number of tools in response
		logger.debug(`Tools in response: ${tools.length}`);

		while (attempts < MAX_ATTEMPTS) {
			attempts++;
			try {
				// Use the new method that implements proper flow: get system prompt, compress history, format messages
				const formattedMessages = await this.contextManager.getFormattedMessage({
					role: 'user',
					content: userInput,
				});

				// Debug log: Show exactly what messages are being sent to OpenAI
				logger.debug(`Sending ${formattedMessages.length} formatted messages to OpenAI:`, {
					messages: formattedMessages.map((msg, idx) => ({
						index: idx,
						role: msg.role,
						hasContent: !!msg.content,
						hasToolCalls: !!msg.tool_calls,
						toolCallId: msg.tool_call_id,
						name: msg.name,
					})),
				});

				// Call OpenAI API
				const response = await this.openai.chat.completions.create({
					model: this.model,
					messages: formattedMessages,
					tools: attempts === 1 ? tools : [], // Only offer tools on first attempt
					tool_choice: attempts === 1 ? 'auto' : 'none', // Disable tool choice on retry
				});

				logger.silly('OPENAI CHAT COMPLETION RESPONSE: ', JSON.stringify(response, null, 2));

				// Get the response message
				const message = response.choices[0]?.message;
				if (!message) {
					throw new Error('Received empty message from OpenAI API');
				}

				return { message };
			} catch (error) {
				const apiError = error as any;
				logger.error(
					`Error in OpenAI API call (Attempt ${attempts}/${MAX_ATTEMPTS}): ${apiError.message || JSON.stringify(apiError, null, 2)}`,
					{ status: apiError.status, headers: apiError.headers }
				);

				if (apiError.status === 400 && apiError.error?.code === 'context_length_exceeded') {
					logger.warn(
						`Context length exceeded. ContextManager compression might not be sufficient. Error details: ${JSON.stringify(apiError.error)}`
					);
				}

				if (attempts >= MAX_ATTEMPTS) {
					logger.error(`Failed to get response from OpenAI after ${MAX_ATTEMPTS} attempts.`);
					throw error;
				}

				await new Promise(resolve => setTimeout(resolve, 500 * attempts));
			}
		}

		throw new Error('Failed to get response after maximum retry attempts');
	}

	private formatToolsForOpenAI(tools: ToolSet): any[] {
		// Keep the existing implementation
		// Convert the ToolSet object to an array of tools in OpenAI's format
		return Object.entries(tools).map(([name, tool]) => {
			return {
				type: 'function',
				function: {
					name,
					description: tool.description,
					parameters: tool.parameters,
				},
			};
		});
	}
}

```

## äºŒã€LLM å·¥å‚å‡½æ•°çš„åˆ›å»º
1. è¾…åŠ©å‡½æ•°çš„åˆ›å»ºï¼škey å’Œ url çš„è·å–
2. ä¸»å‡½æ•°çš„åˆ›å»ºï¼šåˆ›å»º LLM ç±»çš„æ–¹æ³•

### 2.1ã€URL å’Œ Key çš„è·å–å°è£…
å¯¹äº LLM ç±»çš„å®ä¾‹åŒ–ï¼Œæœ€é‡è¦çš„å°±æ˜¯ä¸¤ä¸ªå‚æ•°ï¼š

+ apiKeyï¼š API å¯†é’¥
+ baseURLï¼šæœåŠ¡ç«¯ç‚¹ URL

åŸºäºæ­¤éœ€æ±‚ï¼Œå®ç°ä¸¤ä¸ªæ–¹æ³•ï¼š

+ extractAPIKey å‡½æ•°ï¼šåŒºåˆ†éœ€è¦å’Œä¸éœ€è¦ api å¯†é’¥çš„ä¾›åº”å•†ï¼Œå¹¶ä¸”è¿”å› APIKey
+ getOpenAICompatibleBaseURL å‡½æ•°ï¼šå¤„ç†ä¸åŒä¾›åº”å•†çš„ URL é…ç½®

```typescript
  function extractApiKey(config: LLMConfig): string {
    const provider = config.provider.toLowerCase();

    // å…APIå¯†é’¥çš„æä¾›å•†
    if (provider === 'ollama' || provider === 'lmstudio' ||
        provider === 'aws' || provider === 'azure') {
      return 'not-required';
    }

    let apiKey = config.apiKey || '';
    if (!apiKey) {
      // è¯¦ç»†çš„é”™è¯¯æç¤º
      const errorMsg = `Error: API key for ${provider} not found`;
      logger.error(errorMsg);
      logger.error(`Please set your ${provider} API key in the config file or .env 
  file`);
      throw new Error(errorMsg);
    }
    return apiKey;
  }

  function getOpenAICompatibleBaseURL(llmConfig: LLMConfig): string {
    // 1. ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„baseURL
    if (llmConfig.baseURL) {
      let baseUrl = llmConfig.baseURL.replace(/\/$/, ''); // ç§»é™¤æœ«å°¾æ–œæ 

      // Ollamaç‰¹æ®Šå¤„ç†ï¼šç¡®ä¿/v1åç¼€
      const provider = llmConfig.provider.toLowerCase();
      if (provider === 'ollama' && !baseUrl.endsWith('/v1') &&
  !baseUrl.endsWith('/api')) {
        baseUrl = baseUrl + '/v1';
      }
      return baseUrl;
    }

    // 2. æ ¹æ®æä¾›å•†è¿”å›é»˜è®¤URL
    const provider = llmConfig.provider.toLowerCase();

    if (provider === 'openrouter') {
      return 'https://openrouter.ai/api/v1';
    }

    if (provider === 'ollama') {
      // ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼
      let baseUrl = env.OLLAMA_BASE_URL || 'http://localhost:11434/v1';
      // ç¡®ä¿v1åç¼€
      if (!baseUrl.endsWith('/v1') && !baseUrl.endsWith('/api')) {
        baseUrl = baseUrl.replace(/\/$/, '') + '/v1';
      }
      return baseUrl;
    }

    if (provider === 'lmstudio') {
      return env.LMSTUDIO_BASE_URL || 'http://localhost:1234/v1';
    }

    // OpenAIçš„ç¯å¢ƒå˜é‡å›é€€
    if (provider === 'openai' && env.OPENAI_BASE_URL) {
      return env.OPENAI_BASE_URL.replace(/\/$/, '');
    }

    return ''; // ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä½¿ç”¨SDKé»˜è®¤
  }
```



### 2.2ã€æ ¸å¿ƒåˆ›å»ºæ–¹æ³•çš„å®ç°
æ ¹æ®ä¼ å…¥ä¸åŒçš„å‚æ•°ï¼Œæ¥å®ä¾‹åŒ–ä¸åŒçš„ç±»

+ ä¼ å…¥ openaiï¼Œå®ä¾‹åŒ– new OpenAIService()
+ ä¼ å…¥ openrouterï¼Œå®ä¾‹åŒ– new OpenRouterService()
+ ä¼ å…¥ ollamaï¼Œå®ä¾‹åŒ– new OllamaService()
+ ä¼ å…¥ lmstudioï¼Œå®ä¾‹åŒ– new LMStudioService()
+ ä¼ å…¥ qwenï¼Œå®ä¾‹åŒ– new QwenService()
+ ä¼ å…¥ anthropicï¼Œå®ä¾‹åŒ– new AnthropicService()
+ ä¼ å…¥ awsï¼Œå®ä¾‹åŒ– new AwsService()
+ ä¼ å…¥ azureï¼Œå®ä¾‹åŒ– new AzureService()
+ ä¼ å…¥ geminiï¼Œå®ä¾‹åŒ– new GeminiService()



è®¾è®¡æ ¸å¿ƒå°±æ˜¯ï¼š

+ **å¦‚æœèƒ½ä½¿ç”¨ OpenAI SDK å°±ä½¿ç”¨è¿™ä¸ªï¼Œå¯ä»¥å¤§å¹…å‡å°‘ä¾èµ–**
+ Anthropic æ˜¯ä¸“ç”¨çš„ SDK
+ Gemini ä¹Ÿæ˜¯ä¸“ç”¨çš„ SDK



```typescript
function _createLLMService(
	config: LLMConfig,
	mcpManager: MCPManager,
	contextManager: ContextManager,
	unifiedToolManager?: UnifiedToolManager
): ILLMService {
	// Extract and validate API key
	const apiKey = extractApiKey(config);

	switch (config.provider.toLowerCase()) {
		case 'openai': {
			const baseURL = getOpenAICompatibleBaseURL(config);
			// Use require for OpenAI SDK for compatibility
			// @ts-ignore

			const OpenAIClass = require('openai');
			const openai = new OpenAIClass({ apiKey, ...(baseURL ? { baseURL } : {}) });
			return new OpenAIService(
				openai,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				unifiedToolManager
			);
		}
		case 'openrouter': {
			const baseURL = getOpenAICompatibleBaseURL(config);
			// Use require for OpenAI SDK for compatibility
			// @ts-ignore

			const OpenAIClass = require('openai');
			const openai = new OpenAIClass({
				apiKey,
				baseURL,
				defaultHeaders: {
					'HTTP-Referer': 'https://github.com/byterover/cipher',
					'X-Title': 'Cipher Memory Agent',
				},
			});
			return new OpenRouterService(
				openai,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				unifiedToolManager
			);
		}
		case 'lmstudio': {
			const baseURL = getOpenAICompatibleBaseURL(config);
			// Use require for OpenAI SDK for compatibility
			// @ts-ignore

			const OpenAIClass = require('openai');
			const openai = new OpenAIClass({
				apiKey: 'lm-studio', // LM Studio uses "lm-studio" as the API key
				baseURL,
			});
			return new LMStudioService(
				openai,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				unifiedToolManager
			);
		}
		case 'anthropic': {
			// Use require for Anthropic SDK for compatibility
			// @ts-ignore

			const AnthropicClass = require('@anthropic-ai/sdk');
			const anthropic = new AnthropicClass({ apiKey });
			return new AnthropicService(
				anthropic,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				unifiedToolManager
			);
		}
		case 'ollama': {
			const baseURL = getOpenAICompatibleBaseURL(config);
			// Use require for OpenAI SDK for compatibility
			// @ts-ignore

			const OpenAIClass = require('openai');
			// Ollama uses OpenAI-compatible API but runs locally
			const openai = new OpenAIClass({
				apiKey: 'not-required', // Ollama doesn't require an API key
				baseURL,
			});
			return new OllamaService(
				openai,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				unifiedToolManager
			);
		}
		case 'aws': {
			return new AwsService(
				config.model,
				mcpManager,
				contextManager,
				unifiedToolManager,
				config.maxIterations,
				config.aws
			);
		}
		case 'azure': {
			return new AzureService(
				config.model,
				mcpManager,
				contextManager,
				unifiedToolManager,
				config.maxIterations,
				config.azure
			);
		}
		case 'qwen': {
			// QwenService: OpenAI-compatible endpoint for Alibaba Cloud Qwen
			// Accepts Qwen-specific options via config.qwenOptions
			// Default endpoint: https://dashscope-intl.aliyuncs.com/compatible-mode/v1
			const baseURL = config.baseURL || 'https://dashscope-intl.aliyuncs.com/compatible-mode/v1';
			// Use require for OpenAI SDK for compatibility
			// @ts-ignore

			const OpenAIClass = require('openai');
			const openai = new OpenAIClass({ apiKey, baseURL });
			const qwenOptions: QwenOptions = {
				...(config.qwenOptions?.enableThinking !== undefined && {
					enableThinking: config.qwenOptions.enableThinking,
				}),
				...(config.qwenOptions?.thinkingBudget !== undefined && {
					thinkingBudget: config.qwenOptions.thinkingBudget,
				}),
				...(config.qwenOptions?.temperature !== undefined && {
					temperature: config.qwenOptions.temperature,
				}),
				...(config.qwenOptions?.top_p !== undefined && { top_p: config.qwenOptions.top_p }),
			};
			return new QwenService(
				openai,
				config.model,
				mcpManager,
				contextManager,
				config.maxIterations,
				qwenOptions,
				unifiedToolManager
			);
		}
		case 'gemini': {
			logger.debug('Creating Gemini service', { model: config.model, hasApiKey: !!apiKey });
			try {
				return new GeminiService(
					apiKey,
					config.model,
					mcpManager,
					contextManager,
					config.maxIterations,
					unifiedToolManager
				);
			} catch (error) {
				logger.error('Failed to create Gemini service', {
					error: error instanceof Error ? error.message : String(error),
					model: config.model,
				});
				throw error;
			}
		}
		default:
			throw new Error(`Unsupported LLM provider: ${config.provider}`);
	}
}
```

## ä¸‰ã€æ¶ˆæ¯æ ¼å¼åŒ–å™¨
![](https://cdn.nlark.com/yuque/0/2025/png/29674456/1757262665033-12c578b7-af03-44f4-8c74-42f3b0a5abd6.png)



1ã€ ç»Ÿä¸€çš„å†…éƒ¨æ ¼å¼ä¸ºï¼š

```typescript
interface InternalMessage {
    role: 'system' | 'user' | 'assistant' | 'tool';
    content: string | null | Array<TextSegment | ImageSegment>;  // æ”¯æŒå¤šæ¨¡æ€
    toolCalls?: Array<{...}>;    // å·¥å…·è°ƒç”¨
    toolCallId?: string;         // å·¥å…·ç»“æœID
    name?: string;               // å·¥å…·åç§°
  }
```

é‡Œé¢å¯¹äºå·¥å…·éƒ¨åˆ†çš„å®šä¹‰å‚æ•°

+ toolCallsï¼šæ˜¯æ¨¡å‹å‘èµ·å·¥å…·è°ƒç”¨è¯·æ±‚çš„å‚æ•°ï¼Œé‡Œé¢æœ‰å·¥å…·åç§°ã€å‚æ•°ç­‰ä¿¡æ¯
+ toolCallIdï¼šæ˜¯å·¥å…·æ‰§è¡Œç»“æœçš„å”¯ä¸€æ ‡è¯†ç¬¦
+ nameï¼šè¢«è°ƒç”¨çš„å·¥å…·åç§°



2ã€ ä¾›åº”å•†ç‰¹å®šçš„æ ¼å¼ï¼š

```typescript
  // 1. å·¥å‚å‡½æ•°æ ¹æ®provideré€‰æ‹©æ ¼å¼åŒ–å™¨
  function getFormatter(provider: string): IMessageFormatter {
    switch (provider.toLowerCase()) {
      case 'openai':
      case 'openrouter':
      case 'ollama':
      case 'lmstudio':
      case 'qwen':
      case 'gemini':
        return new OpenAIMessageFormatter();  // 6ä¸ªæä¾›å•†å…±ç”¨

      case 'anthropic':
      case 'aws':
        return new AnthropicMessageFormatter();  // 2ä¸ªæä¾›å•†å…±ç”¨

      case 'azure':
        return new AzureMessageFormatter();
    }
  }
```



3ã€ä½¿ç”¨ç¤ºä¾‹

```typescript
//1ã€åˆ›å»ºæ ¼å¼åŒ–å™¨

const config = { provider: 'openai', model: 'gpt-4' };
const formatter = getFormatter(config.provider);  // OpenAIMessageFormatter
const contextManager = new ContextManager(formatter, ...);

//2ã€æ ¼å¼åŒ–æ¶ˆæ¯

const internalMessage: InternalMessage = {
  role: 'user',
  content: 'Hello, world!'
};

const formatted = formatter.format(internalMessage);
// OpenAI: [{ role: 'user', content: 'Hello, world!' }]
// Anthropic: [{ role: 'user', content: 'Hello, world!' }]

//3ã€å¸¦ç³»ç»Ÿæç¤ºçš„æ ¼å¼åŒ–

const formatted = formatter.format(internalMessage, 'You are a helpful assistant');
// OpenAI: [
//   { role: 'system', content: 'You are a helpful assistant' },
//   { role: 'user', content: 'Hello, world!' }
// ]
// Anthropic: [{ role: 'user', content: 'Hello, world!' }] (ç³»ç»Ÿæç¤ºå•ç‹¬å¤„ç†)
```

